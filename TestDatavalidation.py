#!/usr/bin/env python3
"""
æµ‹è¯•Data Validationç»„ä»¶
"""

import sys
import os
from datetime import datetime

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(project_root)

def test_data_validation():
    """
    æµ‹è¯•Data Validationå®Œæ•´æµç¨‹
    """
    try:
        print("ğŸ” å¼€å§‹æµ‹è¯•Data Validationç»„ä»¶")
        print("=" * 60)
        
        # å¯¼å…¥å¿…è¦çš„æ¨¡å—
        from networksecurity.entity.config_entity import TrainingPipelineConfig, DataIngestionConfig, DataValidationConfig
        from networksecurity.components.data_ingestion import DataIngestion
        from networksecurity.components.data_validation import DataValidation
        from networksecurity.exception.exception import NetworkSecurityException
        from networksecurity.logging.logger import logging
        
        print("âœ… æ‰€æœ‰æ¨¡å—å¯¼å…¥æˆåŠŸ")
        
        # æ­¥éª¤1: åˆ›å»ºé…ç½®å¯¹è±¡
        print("\nğŸ“‹ æ­¥éª¤1: åˆ›å»ºé…ç½®å¯¹è±¡")
        training_pipeline_config = TrainingPipelineConfig()
        data_ingestion_config = DataIngestionConfig(training_pipeline_config)
        data_validation_config = DataValidationConfig(training_pipeline_config)
        
        print(f"   - æ•°æ®éªŒè¯ç›®å½•: {data_validation_config.data_validation_dir}")
        print(f"   - æœ‰æ•ˆæ•°æ®ç›®å½•: {data_validation_config.valid_data_dir}")
        print(f"   - æ— æ•ˆæ•°æ®ç›®å½•: {data_validation_config.invalid_data_dir}")
        print(f"   - æ¼‚ç§»æŠ¥å‘Šè·¯å¾„: {data_validation_config.drift_report_file_path}")
        
        # æ­¥éª¤2: å…ˆæ‰§è¡Œæ•°æ®æ‘„å–ï¼ˆç¡®ä¿æœ‰æ•°æ®å¯ç”¨ï¼‰
        print("\nğŸ“¥ æ­¥éª¤2: æ‰§è¡Œæ•°æ®æ‘„å–ï¼ˆè·å–è®­ç»ƒæµ‹è¯•æ•°æ®ï¼‰")
        data_ingestion = DataIngestion(data_ingestion_config)
        data_ingestion_artifact = data_ingestion.initiate_data_ingestion()
        
        print(f"   - è®­ç»ƒæ–‡ä»¶: {data_ingestion_artifact.trained_file_path}")
        print(f"   - æµ‹è¯•æ–‡ä»¶: {data_ingestion_artifact.test_file_path}")
        
        # éªŒè¯æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(data_ingestion_artifact.trained_file_path):
            raise Exception(f"è®­ç»ƒæ–‡ä»¶ä¸å­˜åœ¨: {data_ingestion_artifact.trained_file_path}")
        if not os.path.exists(data_ingestion_artifact.test_file_path):
            raise Exception(f"æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨: {data_ingestion_artifact.test_file_path}")
            
        print("âœ… æ•°æ®æ‘„å–å®Œæˆï¼Œæ–‡ä»¶éªŒè¯é€šè¿‡")
        
        # æ­¥éª¤3: åˆ›å»ºæ•°æ®éªŒè¯å¯¹è±¡
        print("\nğŸ” æ­¥éª¤3: åˆ›å»ºæ•°æ®éªŒè¯å¯¹è±¡")
        data_validation = DataValidation(
            data_ingestion_artifact=data_ingestion_artifact,
            data_validation_config=data_validation_config
        )
        print("âœ… æ•°æ®éªŒè¯å¯¹è±¡åˆ›å»ºæˆåŠŸ")
        
        # æ­¥éª¤4: æ‰§è¡Œæ•°æ®éªŒè¯
        print("\nğŸ”„ æ­¥éª¤4: æ‰§è¡Œæ•°æ®éªŒè¯æµç¨‹")
        print("   è¿™å°†æ‰§è¡Œä»¥ä¸‹æ£€æŸ¥:")
        print("   1. éªŒè¯åˆ—æ•°æ˜¯å¦æ­£ç¡®")
        print("   2. éªŒè¯æ•°å€¼åˆ—æ˜¯å¦å­˜åœ¨")
        print("   3. éªŒè¯ç›®æ ‡åˆ—æ˜¯å¦æ­£ç¡®")
        print("   4. æ£€æµ‹æ•°æ®æ¼‚ç§»")
        
        # å¼€å§‹éªŒè¯
        data_validation_artifact = data_validation.initiate_data_validation()
        
        print("âœ… æ•°æ®éªŒè¯æµç¨‹å®Œæˆ!")
        
        # æ­¥éª¤5: åˆ†æéªŒè¯ç»“æœ
        print("\nğŸ“Š æ­¥éª¤5: åˆ†æéªŒè¯ç»“æœ")
        print(f"   - éªŒè¯çŠ¶æ€: {'é€šè¿‡' if data_validation_artifact.validation_status else 'å¤±è´¥'}")
        
        if data_validation_artifact.validation_status:
            print("âœ… æ•°æ®éªŒè¯é€šè¿‡!")
            print(f"   - æœ‰æ•ˆè®­ç»ƒæ–‡ä»¶: {data_validation_artifact.valid_train_file_path}")
            print(f"   - æœ‰æ•ˆæµ‹è¯•æ–‡ä»¶: {data_validation_artifact.valid_test_file_path}")
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦çœŸçš„å­˜åœ¨
            if os.path.exists(data_validation_artifact.valid_train_file_path):
                import pandas as pd
                valid_train_df = pd.read_csv(data_validation_artifact.valid_train_file_path)
                print(f"   - æœ‰æ•ˆè®­ç»ƒæ•°æ®: {len(valid_train_df)} è¡Œ, {len(valid_train_df.columns)} åˆ—")
            
            if os.path.exists(data_validation_artifact.valid_test_file_path):
                valid_test_df = pd.read_csv(data_validation_artifact.valid_test_file_path)
                print(f"   - æœ‰æ•ˆæµ‹è¯•æ•°æ®: {len(valid_test_df)} è¡Œ, {len(valid_test_df.columns)} åˆ—")
                
        else:
            print("âŒ æ•°æ®éªŒè¯å¤±è´¥!")
            print(f"   - æ— æ•ˆè®­ç»ƒæ–‡ä»¶: {data_validation_artifact.invalid_train_file_path}")
            print(f"   - æ— æ•ˆæµ‹è¯•æ–‡ä»¶: {data_validation_artifact.invalid_test_file_path}")
        
        # æ­¥éª¤6: æ£€æŸ¥æ¼‚ç§»æŠ¥å‘Š
        print("\nğŸ“ˆ æ­¥éª¤6: æ£€æŸ¥æ•°æ®æ¼‚ç§»æŠ¥å‘Š")
        drift_report_path = data_validation_artifact.drift_report_file_path
        
        if os.path.exists(drift_report_path):
            print(f"âœ… æ¼‚ç§»æŠ¥å‘Šå·²ç”Ÿæˆ: {drift_report_path}")
            
            # è¯»å–å¹¶æ˜¾ç¤ºæŠ¥å‘Šæ‘˜è¦
            from networksecurity.utils.main_utils.utils import read_yaml_file
            drift_report = read_yaml_file(drift_report_path)
            
            if 'summary' in drift_report:
                summary = drift_report['summary']
                print(f"   - æ£€æŸ¥çš„åˆ—æ•°: {summary.get('total_columns_checked', 0)}")
                print(f"   - æ£€æµ‹åˆ°æ¼‚ç§»: {'æ˜¯' if summary.get('drift_detected', False) else 'å¦'}")
                print(f"   - ä½¿ç”¨çš„é˜ˆå€¼: {summary.get('threshold_used', 0.05)}")
                
                if summary.get('drift_columns'):
                    print(f"   - æ¼‚ç§»çš„åˆ—: {summary['drift_columns']}")
                else:
                    print("   - æ²¡æœ‰åˆ—å‡ºç°æ˜¾è‘—æ¼‚ç§»")
            
            # æ˜¾ç¤ºå‰å‡ åˆ—çš„è¯¦ç»†ä¿¡æ¯
            print("   å‰5åˆ—çš„på€¼:")
            count = 0
            for column, info in drift_report.items():
                if isinstance(info, dict) and 'p_value' in info and count < 5:
                    print(f"     {column}: p_value={info['p_value']:.4f}, drift={info['drift_status']}")
                    count += 1
        else:
            print("âŒ æ¼‚ç§»æŠ¥å‘Šæ–‡ä»¶æœªç”Ÿæˆ")
        
        # æ­¥éª¤7: æ€»ç»“
        print("\nğŸ‰ æµ‹è¯•æ€»ç»“")
        print("=" * 60)
        print("âœ… Data Validationç»„ä»¶æµ‹è¯•æˆåŠŸ!")
        print("âœ… æ‰€æœ‰éªŒè¯åŠŸèƒ½æ­£å¸¸å·¥ä½œ")
        print("âœ… æ•°æ®è´¨é‡æ£€æŸ¥å®Œæˆ")
        print("âœ… æ¼‚ç§»æ£€æµ‹æ­£å¸¸è¿è¡Œ")
        
        if data_validation_artifact.validation_status:
            print("\nğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
            print(f"   - æœ‰æ•ˆè®­ç»ƒæ•°æ®: {data_validation_artifact.valid_train_file_path}")
            print(f"   - æœ‰æ•ˆæµ‹è¯•æ•°æ®: {data_validation_artifact.valid_test_file_path}")
        
        print(f"   - æ¼‚ç§»æŠ¥å‘Š: {data_validation_artifact.drift_report_file_path}")
        
        print("\nğŸ¯ ä¸‹ä¸€æ­¥:")
        if data_validation_artifact.validation_status:
            print("   æ•°æ®éªŒè¯é€šè¿‡ï¼Œå¯ä»¥ç»§ç»­è¿›è¡Œæ•°æ®è½¬æ¢ (Data Transformation)")
        else:
            print("   æ•°æ®éªŒè¯å¤±è´¥ï¼Œéœ€è¦æ£€æŸ¥æ•°æ®è´¨é‡é—®é¢˜")
        
        return data_validation_artifact
        
    except NetworkSecurityException as nse:
        print(f"\nâŒ ç½‘ç»œå®‰å…¨å¼‚å¸¸: {str(nse)}")
        return None
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        print("è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
        print(traceback.format_exc())
        return None

def test_individual_validations():
    """
    æµ‹è¯•å„ä¸ªéªŒè¯åŠŸèƒ½
    """
    try:
        print("\nğŸ§ª æµ‹è¯•å„ä¸ªéªŒè¯åŠŸèƒ½")
        print("-" * 40)
        
        # å¯¼å…¥æ¨¡å—
        from networksecurity.entity.config_entity import TrainingPipelineConfig, DataIngestionConfig, DataValidationConfig
        from networksecurity.components.data_ingestion import DataIngestion
        from networksecurity.components.data_validation import DataValidation
        import pandas as pd
        
        # åˆ›å»ºé…ç½®
        training_config = TrainingPipelineConfig()
        data_ingestion_config = DataIngestionConfig(training_config)
        data_validation_config = DataValidationConfig(training_config)
        
        # è·å–æ•°æ®
        data_ingestion = DataIngestion(data_ingestion_config)
        data_ingestion_artifact = data_ingestion.initiate_data_ingestion()
        
        # åˆ›å»ºéªŒè¯å™¨
        data_validation = DataValidation(data_ingestion_artifact, data_validation_config)
        
        # è¯»å–æ•°æ®
        train_df = pd.read_csv(data_ingestion_artifact.trained_file_path)
        test_df = pd.read_csv(data_ingestion_artifact.test_file_path)
        
        print(f"è®­ç»ƒæ•°æ®å½¢çŠ¶: {train_df.shape}")
        print(f"æµ‹è¯•æ•°æ®å½¢çŠ¶: {test_df.shape}")
        
        # æµ‹è¯•1: åˆ—æ•°éªŒè¯
        print("\n1. æµ‹è¯•åˆ—æ•°éªŒè¯:")
        result = data_validation.validate_number_of_columns(train_df)
        print(f"   è®­ç»ƒæ•°æ®åˆ—æ•°éªŒè¯: {'é€šè¿‡' if result else 'å¤±è´¥'}")
        
        result = data_validation.validate_number_of_columns(test_df)
        print(f"   æµ‹è¯•æ•°æ®åˆ—æ•°éªŒè¯: {'é€šè¿‡' if result else 'å¤±è´¥'}")
        
        # æµ‹è¯•2: æ•°å€¼åˆ—éªŒè¯
        print("\n2. æµ‹è¯•æ•°å€¼åˆ—éªŒè¯:")
        result = data_validation.validate_numerical_columns(train_df)
        print(f"   è®­ç»ƒæ•°æ®æ•°å€¼åˆ—éªŒè¯: {'é€šè¿‡' if result else 'å¤±è´¥'}")
        
        result = data_validation.validate_numerical_columns(test_df)
        print(f"   æµ‹è¯•æ•°æ®æ•°å€¼åˆ—éªŒè¯: {'é€šè¿‡' if result else 'å¤±è´¥'}")
        
        # æµ‹è¯•3: ç›®æ ‡åˆ—éªŒè¯
        print("\n3. æµ‹è¯•ç›®æ ‡åˆ—éªŒè¯:")
        result = data_validation.validate_target_column(train_df)
        print(f"   è®­ç»ƒæ•°æ®ç›®æ ‡åˆ—éªŒè¯: {'é€šè¿‡' if result else 'å¤±è´¥'}")
        
        result = data_validation.validate_target_column(test_df)
        print(f"   æµ‹è¯•æ•°æ®ç›®æ ‡åˆ—éªŒè¯: {'é€šè¿‡' if result else 'å¤±è´¥'}")
        
        # æµ‹è¯•4: æ•°æ®æ¼‚ç§»æ£€æµ‹
        print("\n4. æµ‹è¯•æ•°æ®æ¼‚ç§»æ£€æµ‹:")
        drift_result = data_validation.detect_dataset_drift(train_df, test_df)
        print(f"   æ•°æ®æ¼‚ç§»æ£€æµ‹: {'æ— æ˜¾è‘—æ¼‚ç§»' if drift_result else 'æ£€æµ‹åˆ°æ¼‚ç§»'}")
        
        print("âœ… å„ä¸ªéªŒè¯åŠŸèƒ½æµ‹è¯•å®Œæˆ")
        
    except Exception as e:
        print(f"âŒ ä¸ªåˆ«åŠŸèƒ½æµ‹è¯•å¤±è´¥: {str(e)}")

def main():
    """
    ä¸»å‡½æ•°
    """
    print("Data Validation ç»„ä»¶æµ‹è¯•")
    print("=" * 60)
    
    # å®Œæ•´æµç¨‹æµ‹è¯•
    artifact = test_data_validation()
    
    if artifact:
        # ä¸ªåˆ«åŠŸèƒ½æµ‹è¯•
        test_individual_validations()
        
        print("\nğŸŠ æ‰€æœ‰æµ‹è¯•å®Œæˆ!")
        print("ğŸš€ Data Validationç»„ä»¶å·²å‡†å¤‡å°±ç»ª!")
        return True
    else:
        print("\nâŒ æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®å’Œæ•°æ®")
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)