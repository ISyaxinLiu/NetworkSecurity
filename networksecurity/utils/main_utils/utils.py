import yaml
from networksecurity.exception.exception import NetworkSecurityException
from networksecurity.logging.logger import logging
import os,sys
import numpy as np
import dill
import pickle

from sklearn.metrics import r2_score
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# æ–°å¢å¯¼å…¥ï¼šOptunaå’Œå¹¶è¡ŒåŒ–
try:
    import optuna
    from optuna.samplers import TPESampler
    from optuna.pruners import MedianPruner
    OPTUNA_AVAILABLE = True
except ImportError:
    OPTUNA_AVAILABLE = False
    logging.warning("Optuna not available, falling back to GridSearchCV")

from joblib import Parallel, delayed
import warnings
warnings.filterwarnings('ignore')

def read_yaml_file(file_path: str) -> dict:
    try:
        with open(file_path, "rb") as yaml_file:
            return yaml.safe_load(yaml_file)
    except Exception as e:
        raise NetworkSecurityException(e, sys) from e
    
def write_yaml_file(file_path: str, content: object, replace: bool = False) -> None:
    try:
        if replace:
            if os.path.exists(file_path):
                os.remove(file_path)
        os.makedirs(os.path.dirname(file_path), exist_ok=True)
        with open(file_path, "w") as file:
            yaml.dump(content, file)
    except Exception as e:
        raise NetworkSecurityException(e, sys)
    
def save_numpy_array_data(file_path: str, array: np.array):
    """
    Save numpy array data to file
    file_path: str location of file to save
    array: np.array data to save
    """
    try:
        dir_path = os.path.dirname(file_path)
        os.makedirs(dir_path, exist_ok=True)
        with open(file_path, "wb") as file_obj:
            np.save(file_obj, array)
    except Exception as e:
        raise NetworkSecurityException(e, sys) from e
    
def save_object(file_path: str, obj: object) -> None:
    try:
        logging.info("Entered the save_object method of MainUtils class")
        os.makedirs(os.path.dirname(file_path), exist_ok=True)
        with open(file_path, "wb") as file_obj:
            pickle.dump(obj, file_obj)
        logging.info("Exited the save_object method of MainUtils class")
    except Exception as e:
        raise NetworkSecurityException(e, sys) from e
    
def load_object(file_path: str, ) -> object:
    try:
        if not os.path.exists(file_path):
            raise Exception(f"The file: {file_path} is not exists")
        with open(file_path, "rb") as file_obj:
            return pickle.load(file_obj)
    except Exception as e:
        raise NetworkSecurityException(e, sys) from e
    
def load_numpy_array_data(file_path: str) -> np.array:
    """
    load numpy array data from file
    file_path: str location of file to load
    return: np.array data loaded
    """
    try:
        with open(file_path, "rb") as file_obj:
            return np.load(file_obj)
    except Exception as e:
        raise NetworkSecurityException(e, sys) from e

def evaluate_models(X_train, y_train,X_test,y_test,models,param):
    try:
        report = {}

        for i in range(len(list(models))):
            model = list(models.values())[i]
            para=param[list(models.keys())[i]]

            gs = GridSearchCV(model,para,cv=3)
            gs.fit(X_train,y_train)

            model.set_params(**gs.best_params_)
            model.fit(X_train,y_train)

            #model.fit(X_train, y_train)  # Train model

            y_train_pred = model.predict(X_train)

            y_test_pred = model.predict(X_test)

            train_model_score = r2_score(y_train, y_train_pred)

            test_model_score = r2_score(y_test, y_test_pred)

            report[list(models.keys())[i]] = test_model_score

        return report

    except Exception as e:
        raise NetworkSecurityException(e, sys)

def evaluate_classification_models(X_train, y_train, X_test, y_test, models, param):
    """
    ä¸“é—¨ç”¨äºåˆ†ç±»ä»»åŠ¡çš„æ¨¡å‹è¯„ä¼°å‡½æ•° - åŸç‰ˆGridSearch
    é€‚åˆç½‘ç»œå®‰å…¨é’“é±¼æ£€æµ‹é¡¹ç›®
    """
    try:
        report = {}
        
        for i in range(len(list(models))):
            model_name = list(models.keys())[i]
            model = list(models.values())[i]
            para = param[model_name]
            
            # ç½‘æ ¼æœç´¢
            gs = GridSearchCV(model, para, cv=3, scoring='f1')  # æ”¹ç”¨f1è¯„åˆ†
            gs.fit(X_train, y_train)
            
            # è®­ç»ƒæœ€ä½³æ¨¡å‹
            model.set_params(**gs.best_params_)
            model.fit(X_train, y_train)
            
            # é¢„æµ‹
            y_test_pred = model.predict(X_test)
            
            # è®¡ç®—åˆ†ç±»æŒ‡æ ‡
            accuracy = accuracy_score(y_test, y_test_pred)
            precision = precision_score(y_test, y_test_pred, average='weighted')
            recall = recall_score(y_test, y_test_pred, average='weighted')
            f1 = f1_score(y_test, y_test_pred, average='weighted')
            
            report[model_name] = {
                'accuracy': accuracy,
                'precision_score': precision,  # ä¿æŒä¸åŸä»£ç ä¸€è‡´çš„keyå
                'recall_score': recall,
                'f1_score': f1,
                'best_params': gs.best_params_
            }
        
        return report
        
    except Exception as e:
        raise NetworkSecurityException(e, sys)

# ================== æ–°å¢ï¼šOptunaä¼˜åŒ–ç‰ˆæœ¬ ==================

def _optimize_single_model_optuna(model_name, model, param_space, X_train, y_train, X_test, y_test, n_trials=50):
    """
    å•ä¸ªæ¨¡å‹çš„Optunaä¼˜åŒ– - ç”¨äºå¹¶è¡Œè°ƒç”¨
    """
    try:
        def objective(trial):
            # æ ¹æ®å‚æ•°ç©ºé—´ç”Ÿæˆå‚æ•°
            params = {}
            for param_name, param_config in param_space.items():
                if isinstance(param_config, list):
                    # ç¦»æ•£å‚æ•°
                    if all(isinstance(x, (int, float)) for x in param_config):
                        if all(isinstance(x, int) for x in param_config):
                            params[param_name] = trial.suggest_categorical(param_name, param_config)
                        else:
                            params[param_name] = trial.suggest_categorical(param_name, param_config)
                    else:
                        params[param_name] = trial.suggest_categorical(param_name, param_config)
            
            # åˆ›å»ºæ¨¡å‹
            from copy import deepcopy
            temp_model = deepcopy(model)
            temp_model.set_params(**params)
            
            # è®­ç»ƒå’Œè¯„ä¼°
            temp_model.fit(X_train, y_train)
            y_pred = temp_model.predict(X_test)
            
            # è¿”å›F1åˆ†æ•°
            f1 = f1_score(y_test, y_pred, average='weighted')
            return f1
        
        # åˆ›å»ºstudy
        study = optuna.create_study(
            direction='maximize',
            sampler=TPESampler(seed=42),
            pruner=MedianPruner(n_startup_trials=10)
        )
        
        # ä¼˜åŒ–
        study.optimize(objective, n_trials=n_trials, show_progress_bar=False)
        
        # ç”¨æœ€ä½³å‚æ•°é‡æ–°è®­ç»ƒ
        best_model = model.__class__(**study.best_params)
        best_model.fit(X_train, y_train)
        
        # è®¡ç®—æœ€ç»ˆæŒ‡æ ‡
        y_pred = best_model.predict(X_test)
        
        result = {
            'accuracy': accuracy_score(y_test, y_pred),
            'precision_score': precision_score(y_test, y_pred, average='weighted'),
            'recall_score': recall_score(y_test, y_pred, average='weighted'),
            'f1_score': f1_score(y_test, y_pred, average='weighted'),
            'best_params': study.best_params,
            'n_trials': len(study.trials),
            'best_value': study.best_value
        }
        
        logging.info(f"{model_name} Optunaä¼˜åŒ–å®Œæˆ - F1: {result['f1_score']:.4f}, æœ€ä½³å‚æ•°: {study.best_params}")
        return model_name, result
        
    except Exception as e:
        logging.error(f"{model_name} Optunaä¼˜åŒ–å¤±è´¥: {str(e)}")
        # é™çº§åˆ°GridSearch
        return _optimize_single_model_gridsearch(model_name, model, param_space, X_train, y_train, X_test, y_test)

def _optimize_single_model_gridsearch(model_name, model, param_space, X_train, y_train, X_test, y_test):
    """
    å•ä¸ªæ¨¡å‹çš„GridSearchä¼˜åŒ– - å¤‡ç”¨æ–¹æ¡ˆ
    """
    try:
        # è½¬æ¢å‚æ•°æ ¼å¼
        if isinstance(param_space, dict):
            param_grid = param_space
        else:
            param_grid = param_space
            
        gs = GridSearchCV(model, param_grid, cv=3, scoring='f1_weighted')
        gs.fit(X_train, y_train)
        
        y_pred = gs.predict(X_test)
        
        result = {
            'accuracy': accuracy_score(y_test, y_pred),
            'precision_score': precision_score(y_test, y_pred, average='weighted'),
            'recall_score': recall_score(y_test, y_pred, average='weighted'),
            'f1_score': f1_score(y_test, y_pred, average='weighted'),
            'best_params': gs.best_params_
        }
        
        logging.info(f"{model_name} GridSearchå®Œæˆ - F1: {result['f1_score']:.4f}")
        return model_name, result
        
    except Exception as e:
        logging.error(f"{model_name} GridSearchå¤±è´¥: {str(e)}")
        raise e

def evaluate_classification_models_parallel(X_train, y_train, X_test, y_test, models, param, 
                                           use_optuna=True, n_trials=50, n_jobs=-1):
    """
    ğŸ”¥ æ–°å¢ï¼šå¹¶è¡ŒåŒ– + Optunaä¼˜åŒ–çš„åˆ†ç±»æ¨¡å‹è¯„ä¼°
    
    Args:
        X_train, y_train, X_test, y_test: è®­ç»ƒæµ‹è¯•æ•°æ®
        models: æ¨¡å‹å­—å…¸
        param: å‚æ•°å­—å…¸
        use_optuna: æ˜¯å¦ä½¿ç”¨Optunaï¼ˆé»˜è®¤Trueï¼‰
        n_trials: Optunaè¯•éªŒæ¬¡æ•°
        n_jobs: å¹¶è¡Œè¿›ç¨‹æ•°ï¼ˆ-1ä½¿ç”¨æ‰€æœ‰CPUï¼‰
    
    Returns:
        dict: æ¨¡å‹è¯„ä¼°æŠ¥å‘Š
    """
    try:
        logging.info(f"å¼€å§‹å¹¶è¡Œæ¨¡å‹è¯„ä¼° - ä½¿ç”¨Optuna: {use_optuna and OPTUNA_AVAILABLE}, å¹¶è¡Œåº¦: {n_jobs}")
        
        if n_jobs == -1:
            import multiprocessing
            n_jobs = min(multiprocessing.cpu_count(), len(models))
        
        # é€‰æ‹©ä¼˜åŒ–å‡½æ•°
        if use_optuna and OPTUNA_AVAILABLE:
            optimize_func = _optimize_single_model_optuna
            extra_args = (n_trials,)
            logging.info(f"ä½¿ç”¨Optunaè´å¶æ–¯ä¼˜åŒ–ï¼Œæ¯ä¸ªæ¨¡å‹{n_trials}æ¬¡è¯•éªŒ")
        else:
            optimize_func = _optimize_single_model_gridsearch
            extra_args = ()
            logging.info("ä½¿ç”¨GridSearchç½‘æ ¼æœç´¢")
        
        # å¹¶è¡Œä¼˜åŒ–æ‰€æœ‰æ¨¡å‹
        results = Parallel(n_jobs=n_jobs, backend='threading')(
            delayed(optimize_func)(
                model_name, model, param[model_name], 
                X_train, y_train, X_test, y_test, *extra_args
            ) 
            for model_name, model in models.items()
        )
        
        # æ•´ç†ç»“æœ
        report = {}
        for model_name, result in results:
            report[model_name] = result
        
        # æ‰“å°æ’åºç»“æœ
        sorted_models = sorted(report.items(), key=lambda x: x[1]['f1_score'], reverse=True)
        logging.info("=== æ¨¡å‹æ€§èƒ½æ’åº (æŒ‰F1åˆ†æ•°) ===")
        for i, (name, metrics) in enumerate(sorted_models, 1):
            logging.info(f"{i}. {name}: F1={metrics['f1_score']:.4f}, "
                        f"Precision={metrics['precision_score']:.4f}, "
                        f"Recall={metrics['recall_score']:.4f}")
        
        return report
        
    except Exception as e:
        logging.error(f"å¹¶è¡Œæ¨¡å‹è¯„ä¼°å¤±è´¥ï¼Œé™çº§åˆ°ä¸²è¡ŒGridSearch: {str(e)}")
        return evaluate_classification_models(X_train, y_train, X_test, y_test, models, param)